# -*- coding: utf-8 -*-
"""PROJECT-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hd_JJECLPcEHVYYpKV8-epcVYb0UHB9G
"""

!pip install pandas numpy seaborn matplotlib klib dtale scikit-learn joblib pandas-profiling xgboost

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
# %matplotlib inline     
#magic function in IPython  In[101]
import matplotlib.pyplot as plt     # is a collection of command style functions that make matplotlib work like MATLAB
import seaborn as sns

df_train= pd.read_csv(r'/content/train.csv')
df_test= pd.read_csv(r'/content/test.csv')

df_train.head()

df_train.shape

df_train.isnull().sum()

df_test.isnull().sum()

df_train.info()

df_train.describe()

df_train['Item_Weight'].describe()

df_train['Item_Weight'].fillna(df_train['Item_Weight'].mean(),inplace=True)
df_test['Item_Weight'].fillna(df_train['Item_Weight'].mean(),inplace=True)

df_train.isnull().sum()

df_train['Item_Weight'].describe()

df_train['Outlet_Size']

df_train['Outlet_Size'].value_counts()

df_train['Outlet_Size'].mode()

df_train['Outlet_Size'].fillna(df_train['Outlet_Size'].mode()[0],inplace=True)
df_test['Outlet_Size'].fillna(df_test['Outlet_Size'].mode()[0],inplace=True)

df_train.isnull().sum()

df_test.isnull().sum()

df_train.drop(['Item_Identifier','Outlet_Identifier'],axis=1,inplace=True)
df_test.drop(['Item_Identifier','Outlet_Identifier'],axis=1,inplace=True)

df_train

df_test

import dtale
dtale.show(df_train)

pip install ipywidgets

from pandas_profiling import ProfileReport

profile = ProfileReport(df_train, title ="Pandas Profiling Report")
profile
plt.figure(figsize=(10,5))
sns.heatmap(df_train.corr(),annot=True)
plt.show()

import klib

klib.cat_plot(df_train)

klib.corr_mat(df_train)

klib.corr_plot(df_train)

klib.dist_plot(df_train)

klib.missingval_plot(df_train)

klib.data_cleaning(df_train)

klib.clean_column_names(df_train)

df_train.info()

df_train=klib.convert_datatypes(df_train) # converts existing to more efficient dtypes, also called inside data_cleaning()
df_train.info()

klib.mv_col_handling(df_train)

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()

df_train['item_fat_content']= le.fit_transform(df_train['item_fat_content'])
df_train['item_type']= le.fit_transform(df_train['item_type'])
df_train['outlet_size']= le.fit_transform(df_train['outlet_size'])
df_train['outlet_location_type']= le.fit_transform(df_train['outlet_location_type'])
df_train['outlet_type']= le.fit_transform(df_train['outlet_type'])

X=df_train.drop('item_outlet_sales',axis=1)
Y=df_train['item_outlet_sales']
from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, random_state=101, test_size=0.2)

X.describe()

from sklearn.preprocessing import StandardScaler
sc= StandardScaler()
X_train_std= sc.fit_transform(X_train)  # learning how the data is in X train and then transforming
X_test_std= sc.transform(X_test)
X_train_std

X_test_std

Y_train

Y_test

import joblib 
joblib.dump(sc,r'D:\models\sc.sav')

X_test.head()

from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

from sklearn.linear_model import LinearRegression
lr= LinearRegression()
lr.fit(X_train_std,Y_train)

Y_pred_lr=lr.predict(X_test_std)
r2_score(Y_test,Y_pred_lr)

print(r2_score(Y_test,Y_pred_lr))
print(mean_absolute_error(Y_test,Y_pred_lr))
print(np.sqrt(mean_squared_error(Y_test,Y_pred_lr)))

joblib.dump(lr,r'D:\models\lr.sav')

from sklearn.ensemble import RandomForestRegressor
rf= RandomForestRegressor()
rf.fit(X_train_std,Y_train)

Y_pred_rf= rf.predict(X_test_std)
r2_score(Y_test,Y_pred_rf)

print(r2_score(Y_test,Y_pred_rf))
print(mean_absolute_error(Y_test,Y_pred_rf))
print(np.sqrt(mean_squared_error(Y_test,Y_pred_rf)))

joblib.dump(rf,r'D:\models\rf.sav')

from xgboost import XGBRegressor
xg= XGBRegressor()
xg.fit(X_train_std, Y_train)

Y_pred_xg= xg.predict(X_test_std)
r2_score(Y_test,Y_pred_xg)

print(r2_score(Y_test,Y_pred_xg))
print(mean_absolute_error(Y_test,Y_pred_xg))
print(np.sqrt(mean_squared_error(Y_test,Y_pred_xg)))

joblib.dump(rf,r'D:\models\xg.sav')

from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV

# define models and parameters
model = RandomForestRegressor()
n_estimators = [10, 100, 1000]
max_depth=range(1,31)
min_samples_leaf=np.linspace(0.1, 1.0)
max_features=["auto", "sqrt", "log2"]
min_samples_split=np.linspace(0.1, 1.0, 10)

# define grid search
grid = dict(n_estimators=n_estimators)


grid_search_forest = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, 
                           scoring='r2',error_score=0,verbose=2,cv=2)

grid_search_forest.fit(X_train_std, Y_train)

# summarize results
print(f"Best: {grid_search_forest.best_score_:.3f} using {grid_search_forest.best_params_}")
means = grid_search_forest.cv_results_['mean_test_score']
stds = grid_search_forest.cv_results_['std_test_score']
params = grid_search_forest.cv_results_['params']

for mean, stdev, param in zip(means, stds, params):
    print(f"{mean:.3f} ({stdev:.3f}) with: {param}")

grid_search_forest.best_params_

grid_search_forest.best_score_

Y_pred_rf_grid=grid_search_forest.predict(X_test_std)
r2_score(Y_test,Y_pred_rf_grid)

import joblib
joblib.dump(grid_search_forest,r'D:\random_forest_grid.sav')

model=joblib.load(r'D:\random_forest_grid.sav')
model.predict(X_test_std)